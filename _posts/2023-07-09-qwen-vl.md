---
layout: post
title: Qwen-VL A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond
tags:  global-attention bert clip pali multimodal llm blip2 object-detection blip query-transformer vit align qformer ablef deep-learning window-attention coca
---

This is my reading note for [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://github.com/QwenLM/Qwen-VL). This paper proposes a vision-language model capable of vision grounding and image text reading. To do that, it considers visual grounding and OCR tasks in pre-training. In architecture, the paper uses Qformer from BLIP2.

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-1-x66-y97.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-2-x65-y381.png) 

# Introduction
Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. [(p. 1)](zotero://open-pdf/library/items/FRTKJQY2?page=1&annotation=NX9Q48EH)

the majority of open-source LVLMs remain perceiving the image in a coarse-grained approach and lacking the ability to execute fine-grained perception such as object grounding or text reading. [(p. 2)](zotero://open-pdf/library/items/FRTKJQY2?page=2&annotation=7426E56K)

We empower the LLM basement with visual capacity by introducing a new visual receptor including a language-aligned visual encoder and a positionaware adapter. [(p. 3)](zotero://open-pdf/library/items/FRTKJQY2?page=3&annotation=WJF3J8D7)

# Methodology
## Model Architecture
### Visual Encoder
The visual encoder of Qwen-VL uses the Vision Transformer (ViT) (Dosovitskiy et al., 2021) architecture, initialized with pre-trained weights from Openclip’s ViT-bigG (Ilharco et al., 2021). During both training and inference, input images are resized to a specific resolution. The visual encoder processes images by splitting them into patches with a stride of 14, generating a set of image features. [(p. 3)](zotero://open-pdf/library/items/FRTKJQY2?page=3&annotation=VMTWMLNN)

### Position-aware Vision-Language Adapter
To alleviate the efficiency issues arising from long image feature sequences, Qwen-VL introduces a vision-language adapter that compresses the image features. This adapter comprises a single-layer cross-attention module initialized randomly. The module uses a group of trainable vectors (Embeddings) as query vectors and the image features from the visual encoder as keys for crossattention operations. This mechanism compresses the visual feature sequence to a fixed length of 256. [(p. 3)](zotero://open-pdf/library/items/FRTKJQY2?page=3&annotation=36ZGFVDD)

2D absolute positional encodings are incorporated into the cross-attention mechanism’s query-key pairs to mitigate the potential loss of positional details during compression. [(p. 4)](zotero://open-pdf/library/items/FRTKJQY2?page=4&annotation=AMTN2MW5)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-4-x193-y599.png) 

## Inputs and Outputs
### Image Input
Images are processed through the visual encoder and adapter, yielding fixed-length sequences of image features. To differentiate between image feature input and text feature input, two special tokens (<img> and </img>) are appended to the beginning and end of the image feature sequence respectively, signifying the start and end of image content. [(p. 4)](zotero://open-pdf/library/items/FRTKJQY2?page=4&annotation=HEAPQUTY)

### Bounding Box Input and Output
To enhance the model’s capacity for fine-grained visual understanding and grounding, Qwen-VL’s training involves data in the form of region descriptions, questions, and detections. Differing from conventional tasks involving image-text descriptions or questions, this task necessitates the model’s accurate understanding and generation of region descriptions in a designated format. For any given bounding box, a normalization process is applied (within the range `[0, 1000)` and transformed into a specified string format: "(Xtopleft, Ytopleft), (Xbottomright, Ybottomright)". The string is tokenized as text and does not require an additional positional vocabulary. To distinguish between detection strings and regular text strings, two special tokens (<box> and </box> are added at the beginning and end of the bounding box string. Additionally, to appropriately associate bounding boxes with their corresponding descriptive words or sentences, another set of special tokens (<ref> and </ref>) is introduced, marking the content referred to by the bounding box. [(p. 4)](zotero://open-pdf/library/items/FRTKJQY2?page=4&annotation=S866WRP3)

# Training
## Pre-Training
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-4-x71-y364.png) 

In the first stage of pre-training, we mainly utilize a large-scale, weakly labeled, web-crawled set of image-text pairs [(p. 5)](zotero://open-pdf/library/items/FRTKJQY2?page=5&annotation=XCRVKJLQ)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-5-x70-y328.png) 

We freeze the large language model and only optimize the vision encoder and VL adapter in this stage. The input images are resized to 224 × 224. The training objective is to minimize the cross-entropy of the text tokens. The maximum learning rate is 2e−4 and the training process uses a batch size of 30720 for the image-text pairs, and the entire first stage of pre-training lasts for 50,000 steps, consuming approximately 1.5 billion image-text samples. [(p. 5)](zotero://open-pdf/library/items/FRTKJQY2?page=5&annotation=26QQ2HE4)

## Multi-task Pre-training
In the second stage of multi-task pre-training, we introduce high-quality and fine-grained VL annotation data with a larger input resolution and interleaved image-text data. As summarized in Table 3, we trained Qwen-VL on 7 tasks simultaneously. [(p. 5)](zotero://open-pdf/library/items/FRTKJQY2?page=5&annotation=YYADTGE5)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-19-x65-y337.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-6-x120-y524.png) 

We increase the input resolution of the visual encoder from 224 × 224 to 448 × 448, reducing the information loss caused by image down-sampling. [(p. 6)](zotero://open-pdf/library/items/FRTKJQY2?page=6&annotation=VTD6TTM3)

## Supervised Fine-tuning
During this stage, we finetuned the Qwen-VL pre-trained model through instruction fine-tuning to enhance its instruction following and dialogue capabilities, resulting in the interactive Qwen-VL-Chat model. The multi-modal instruction tuning data primarily comes from caption data or dialogue data generated through LLM self-instruction, which often only addresses single-image dialogue and reasoning and is limited to image content comprehension. We construct an additional set of dialogue data through manual annotation, model generation, and strategy concatenation to incorporate localization and multi-image comprehension abilities into the Qwen-VL model. [(p. 6)](zotero://open-pdf/library/items/FRTKJQY2?page=6&annotation=QQBAMLSP)

Additionally, we mix multi-modal and pure text dialogue data during training to ensure the model’s universality in dialogue capabilities. The instruction tuning data amounts to 350k. In this stage, we freeze the visual encoder and optimize the language model and adapter module. [(p. 6)](zotero://open-pdf/library/items/FRTKJQY2?page=6&annotation=9QHKJA4J)

# Evaluation
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-7-x95-y547.png) 

## Refer Expression Comprehension
Specifically, the refer expression comprehension task requires the model to localize the target object under the guidance of a description. [(p. 7)](zotero://open-pdf/library/items/FRTKJQY2?page=7&annotation=P6UWRSDB)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-8-x118-y412.png) 

## Few-shot Learning on Vision-Language Tasks
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-8-x71-y84.png) 

## Instruction Following in Real-world User Behavior
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-9-x152-y569.png) 

# Ablation: Window Attention vs Global Attention for Vision Transformer
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/baiQwenVLVersatileVisionLanguage2023-22-x74-y135.png) 

As shown in Figure 8 and Table 10, the loss of the model is significantly higher when Window Attention instead of Vanilla Attention is used. And the training speeds for both of them are similar. Therefore, we decided to use Vanilla Attention instead of Window Attention for the Vision Transformer when training Qwen-VL. [(p. 22)](zotero://open-pdf/library/items/FRTKJQY2?page=22&annotation=6QKLCCBP)
