---
layout: post
title: Neural Radiance Field
tags: deep-learning cnn mlp nerf differential-rendering volume
---

Neural Radiance Field (NeRF), you may have heard words many times for the past few months. Yes, this is the latest progress of neutral work and computer graphics. NeRF represents a scene with learned, continuous volumetric radiance field $$F_{\theta}$$ defined over a bounded 3D volume. In Nerf, $$F_{\theta}$$ is a multilayer perceptron (MLP) that takes as input a 3D position $$x=(x,y,z)$$ and unit-norm viewing direction $$d=(d_x,d_y,d_z)$$, and produces as output a density $$\sigma$$ and color $$c=(r,g,b)$$. By enumerating all most position and direction for a bounded 3D volumne, we could obtain the 3D scene.

The weights of the multilayer perceptron that parameterize $$F_{\theta}$$ are optimized so as to encode the radiance field of the scene. Volume rendering is used to compute the color of a single pixel.

The first well known paper on this topic is [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/pdf/2003.08934v2.pdf). To learn such field,  you only need to capture a few images of the scene from different angles (a), then you try to fit the MLP between your camera pose to the image(b). This is also illustrated in the chart below. 

![image-20220415181139136](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/2022_04_15_18_11_39_image-20220415181139136.png)

For rendering a scene (i.e., inference, c), you need to pick a camera view, which is 3D position and orientation. Then you enumerate the directions according the orientation of the camera and the angle of view of the camera, to estimate the brightness and color for this direction. Putting all together you got an image for this camera view.

# More on Training

The training aims to learn the MLP (weights) such that the volume generated by the MLP could render the images replicating the input images. To this end, MLP will produce color and volume density by enumerating the rays over all position and direction. Then images will be rendered from this color and volume density at the same camera view as the input images. Given this whole process (including rendering) is differentiable, we could optimize the MLP such that the differences of rendered images and input images are minimized.

You may realize the training requires to have the correspondence between the input (3D locations and directions) and the output (pixel intensity and density). The output is easily available from the images. But for the input, 3D locations and directions, you would need to have camera extrinic and intrinsic.

Camera extrinsic could be computed via struction from motion. The NeRF paper recommend using the `imgs2poses.py` script from the [LLFF code](https://github.com/fyusion/llff). Combining camera intrinsic and extrinsic, you could obtain the orientation of each ray from camera to every pixels on the image.

![img](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/2022_04_15_18_12_06_2022_04_15_18_02_46_pipeline.jpg)

# Results

Some results from [bmild/nerf](https://github.com/bmild/nerf):

![ferngif](https://camo.githubusercontent.com/2c1d3f539c2c3b0e67023599847c1b6ed4e47f3a5fd400c0f48ec815cd4e8e73/68747470733a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e626d696c642f6e6572662f6665726e5f3230306b5f323536772e676966)

![legogif](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/2022_04_15_18_07_28_68747470733a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e626d696c642f6e6572662f6c65676f5f3230306b5f323536772e676966.gif)

# Next Steps

Cool, everything sounds great. Why we not use it to replace existing rendering method. While it is too slow. Optimizing a NeRF takes between a few hours and a day or two (depending on resolution) and only requires a single GPU. Rendering an image from an optimized NeRF takes somewhere between less than a second and ~30 seconds, again depending on resolution. But using current mesh + texture way, it won't takes you more than few millisecond.  There have been a lot of efforts made to improve the speed, which could be covered in future posts.

