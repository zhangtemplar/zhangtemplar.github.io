---
layout: post
title: Must-read AI Papers
tags:  word2vec bert capsnet deep-learning deep-q reformer gan resnet alexnet gpt-3
---

I will create a new reading note series based on [Must-read AI Papers](https://crossminds.ai/playlist/6011f07becbeebc970a2ef20/?utm_campaign=41c4d93aa41f8b49&utm_medium=share) from Crossminds.ai.

| Name           | Paper                                                        | Category      |
| -------------- | ------------------------------------------------------------ | ------------- |
| AlexNet        | [AlexNet: ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) | Vision        |
| ResNet         | [ResNet: Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) | Vision        |
| GAN            | [Introduction to Generative Adversarial Networks](https://arxiv.org/abs/1701.00160) | Vision        |
| OpenAI GPT-3   | [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) | NLP           |
| Word2Vec       | [Word2Vec: Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546) | NLP           |
| Transformer    | [Transformer: Attention Is All You Need](https://arxiv.org/abs/1706.03762) | NLP           |
| CapsNet        | [CapsNets: Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829) | General       |
| BERT           | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) | NLP           |
| Deep Q Network | [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) | Reinforcement |
| Reformer       | [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) | NLP           |

