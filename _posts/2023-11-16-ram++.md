---
layout: post
title: Inject Semantic Concepts into Image Tagging for Open-Set Recognition
tags:  object-detection multimodal deep-learning ram tag clip image-tag-alignment swin
---

This is my reading note for [Inject Semantic Concepts into Image Tagging for Open-Set Recognition](https://github.com/xinyu1205/recognize-anything). This paper proposes an image tagging method based on CLIP. The major innovation is the introduction of image tag alignment loss which aligns image feature to the tag description feature. The tag descriptor is generated by LLM to describe the tog in a few sentences

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-1-x303-y325.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-2-x46-y551.png) 

# Introduction
In this paper, we introduce the Recognize Anything Plus Model (RAM++), a fundamental image recognition model with strong open-set recognition capabilities, by injecting semantic concepts into image tagging training framework. [(p. 1)](zotero://open-pdf/library/items/UZ7LJ85V?page=1&annotation=57LJ6AAK)

In contrast, RAM++ integrates image-text alignment and image-tagging within a unified fine-grained interaction framework based on image-tagstext triplets. This design enables RAM++ not only excel in identifying predefined categories, but also significantly augment the recognition ability in open-set categories [(p. 1)](zotero://open-pdf/library/items/UZ7LJ85V?page=1&annotation=GAM44I47)

There are two principal research avenues for image recognition:
1. Image tagging, also known as multi-label image recognition, aims to provide multiple semantic labels within a given image. Predominantly, such models rely on manually annotated datasets with limited scale [12, 27], resulting in only performing well on specific datasets and categories.
2. Vision-Language models based on large scale image-text pairs, which can align various semantic texts with an image. The representative work is CLIP [41], which has powerful open-set recognition capabilities and widely empowers other fields, such as open-set object detection [15], image segmentation [15] and video tasks [32]. [(p. 2)](zotero://open-pdf/library/items/UZ7LJ85V?page=2&annotation=87M9AMQR)

Although CLIP showcases remarkable zero-shot performance in image single label classification [9], its interaction on visual-linguistic features (dot product) is relatively shallow, making it difficult to handle more realistic finegrained image tagging tasks [52]. [(p. 2)](zotero://open-pdf/library/items/UZ7LJ85V?page=2&annotation=E3TBIHRV)

Specifically, we adopt the LLM to automatically generate diverse visual descriptions for each tag category and synthesize into tag embeddings to align with image features. [(p. 2)](zotero://open-pdf/library/items/UZ7LJ85V?page=2&annotation=BZ4B4727)

# Proposed Method
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-3-x37-y430.png) 

To recognize categories beyond the fixed label system, RAM utilizes an off-the-shelf text encoder [41] to extract textual tag embeddings. [(p. 4)](zotero://open-pdf/library/items/UZ7LJ85V?page=4&annotation=DWANMELG)

## Image-Text Alignment
With image-tags-text triplets, RAM only performs image tags alignment for image tagging. To enrich the semantic concepts beyond fixed tag categories, we introduce Image Text Alignment in addition to image-tags alignment within the tagging framework. [(p. 4)](zotero://open-pdf/library/items/UZ7LJ85V?page=4&annotation=QEJB9UWQ)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-4-x295-y570.png) 

For image tagging, the tag categories are fixed but large quantity (over 4500 categories). Although extracting embedding for all categories is time-consuming, the tag embeddings can be extracted offline using an off-the-shelf text encoder. [(p. 4)](zotero://open-pdf/library/items/UZ7LJ85V?page=4&annotation=QL5DEDBH)

Our ITA achieves a balance between performance and efficiency, capable of aligning a single image with thousands of tags or texts with high efficiency. Moreover, by deleting the self-attention layers in the alignment decoder, ITA support any quantity of texts or tags without affecting performance. [(p. 4)](zotero://open-pdf/library/items/UZ7LJ85V?page=4&annotation=PBB5KCG6)

## LLM-Based Tag Description
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-5-x51-y537.png) 

This approach involves generating distinct descriptions for each tag category, leveraging the capabilities of the LLM in the context of image tagging training. [(p. 5)](zotero://open-pdf/library/items/UZ7LJ85V?page=5&annotation=NTGSDS6B)

We anticipate that the tag descriptions generated by LLMs predominantly exhibit two characteristics: 1) As diverse as possible to cover a broader range of scenarios; 2) As relevant as possible to image features for ensuring high relevance. [(p. 5)](zotero://open-pdf/library/items/UZ7LJ85V?page=5&annotation=HX4IZ8CY)

Drawing inspiration from [40], we design a total of five LLM prompts for each tag category, as follows: (1) “Describe concisely what a(n) {} looks like”; (2) “How can you identify a(n) {} concisely?”; (3) “What does a(n) {} look like concisely?”; (4) “What are the identifying characteristics of a(n) {}”; (5) “Please provide a concise description of the visual characteristics of {}”. [(p. 5)](zotero://open-pdf/library/items/UZ7LJ85V?page=5&annotation=APDP72JA)

To promote the diversity of the LLM responses, we set temperature = 0.99. Consequently, we acquired 10 unique responses for each LLM prompt, amassing a total of 50 tag descriptions per category. [(p. 5)](zotero://open-pdf/library/items/UZ7LJ85V?page=5&annotation=TG98R77D)

# Experiment
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-5-x316-y637.png) 

Our label system includes 4,585 categories that commonly used in texts. [(p. 5)](zotero://open-pdf/library/items/UZ7LJ85V?page=5&annotation=E8JE5NRJ)

We employ SwinBase [30] pretrained on ImageNet [9] as the image encoder, and also select base scale models when comparing with other methods for fair comparison. We leverage the off-the-shelf text encoder from CLIP [41] to extract tag and text features. The alignment decoder is a 2-layer transformer [47] with each layer only consist of cross-attention layer and feed-forward layer. We adopt ASL [43] loss function for both image tagging and image-text alignment [(p. 5)](zotero://open-pdf/library/items/UZ7LJ85V?page=5&annotation=JBK96YD4)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-6-x44-y449.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-7-x48-y475.png) 

# Ablation
![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-7-x46-y339.png) 

1. Case (a) demonstrates that the integration of Image-Text Alignment within the image tagging framework can effectively enhance the model’s recognition capabilities in openset categories [(p. 7)](zotero://open-pdf/library/items/UZ7LJ85V?page=7&annotation=RR53WZV3)
2. Case (c) underscores the effectiveness of incorporating LLM-based tag descriptions in the training stage. [(p. 7)](zotero://open-pdf/library/items/UZ7LJ85V?page=7&annotation=4UN6KVSV)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-8-x48-y509.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-8-x304-y602.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-8-x303-y496.png) 

We observe that the LLM description diversity controlled by temperature is characterized by limited semantic diversity yet mostly rephrasing the same sentence. [(p. 8)](zotero://open-pdf/library/items/UZ7LJ85V?page=8&annotation=SUAZUEVZ)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-11-x313-y584.png) 

Image-Text Alignment Loss Function. In Table 9 and Table 10, we compare different alignment loss functions for image-text alignment and image tagging, including the Cross Entropy (CE) function employed by CLIP, and other robust tagging loss functions (BCE, ASL [43], Hill [51], SPLC [51]). The results indicate that ASL outperforms other loss functions, which alleviates the potential missing labels and imbalance between positive and negative samples. [(p. 11)](zotero://open-pdf/library/items/UZ7LJ85V?page=11&annotation=RG7SPENS)

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-11-x296-y88.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-12-x43-y382.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-12-x45-y156.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-12-x305-y403.png) 

![](https://raw.githubusercontent.com/zhangtemplar/zhangtemplar.github.io/master/uPic/huangInjectSemanticConcepts2023-12-x298-y250.png) 
