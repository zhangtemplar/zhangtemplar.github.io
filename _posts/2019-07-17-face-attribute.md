---
layout: post
title: Face Attribute in CVPR 2019
tags:  deep-learning expression-transfer face-attribute style-transfer
---

![](http://www.graphics.stanford.edu/~niessner/papers/2015/10face/teaser.jpg)

# Face Attribute

## [A Compact Embedding for Facial Expression Similarity](http://openaccess.thecvf.com/content_CVPR_2019/html/Vemulapalli_A_Compact_Embedding_for_Facial_Expression_Similarity_CVPR_2019_paper.html)

> [Raviteja Vemulapalli](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Aseem Agarwala](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Vemulapalli_A_Compact_Embedding_for_Facial_Expression_Similarity_CVPR_2019_paper.pdf), [supp](http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Vemulapalli_A_Compact_Embedding_CVPR_2019_supplemental.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> Most of the existing work on automatic facial expression analysis focuses on discrete emotion recognition, or facial action unit detection. However, facial expressions do not always fall neatly into pre-defined semantic categories. Also, the similarity between expressions measured in the action unit space need not correspond to how humans perceive expression similarity. Different from previous work, our goal is to describe facial expressions in a continuous fashion using a compact embedding space that mimics human visual preferences. To achieve this goal, we collect a large-scale faces-in-the-wild dataset with human annotations in the form: Expressions A and B are visually more similar when compared to expression C, and use this dataset to train a neural network that produces a compact (16-dimensional) expression embedding. We experimentally demonstrate that the learned embedding can be successfully used for various applications such as expression retrieval, photo album summarization, and emotion recognition. We also show that the embedding learned using the proposed dataset performs better than several other embeddings learned using existing emotion or action unit datasets.

## [Facial Emotion Distribution Learning by Exploiting Low-Rank Label Correlations Locally](http://openaccess.thecvf.com/content_CVPR_2019/html/Jia_Facial_Emotion_Distribution_Learning_by_Exploiting_Low-Rank_Label_Correlations_Locally_CVPR_2019_paper.html)
> [Xiuyi Jia](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Xiang Zheng](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Weiwei Li](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Changqing Zhang](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Zechao Li](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Jia_Facial_Emotion_Distribution_Learning_by_Exploiting_Low-Rank_Label_Correlations_Locally_CVPR_2019_paper.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> Emotion recognition from facial expressions is an interesting and challenging problem and has attracted much attention in recent years. Substantial previous research has only been able to address the ambiguity of "what describes the expression", which assumes that each facial expression is associated with one or more predefined affective labels while ignoring the fact that multiple emotions always have different intensities in a single picture. Therefore, to depict facial expressions more accurately, this paper adopts a label distribution learning approach for emotion recognition that can address the ambiguity of "how to describe the expression" and proposes an emotion distribution learning method that exploits label correlations locally. Moreover, a local low-rank structure is employed to capture the local label correlations implicitly. Experiments on benchmark facial expression datasets demonstrate that our method can better address the emotion distribution recognition problem than state-of-the-art methods.

## [Attribute-Aware Face Aging With Wavelet-Based Generative Adversarial Networks](http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Attribute-Aware_Face_Aging_With_Wavelet-Based_Generative_Adversarial_Networks_CVPR_2019_paper.html)
> [Yunfan Liu](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Qi Li](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Zhenan Sun](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Attribute-Aware_Face_Aging_With_Wavelet-Based_Generative_Adversarial_Networks_CVPR_2019_paper.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> Since it is difficult to collect face images of the same subject over a long range of age span, most existing face aging methods resort to unpaired datasets to learn age mappings. However, the matching ambiguity between young and aged face images inherent to unpaired training data may lead to unnatural changes of facial attributes during the aging process, which could not be solved by only enforcing identity consistency like most existing studies do. In this paper, we propose an attribute-aware face aging model with wavelet based Generative Adversarial Networks (GANs) to address the above issues. To be specific, we embed facial attribute vectors into both the generator and discriminator of the model to encourage each synthesized elderly face image to be faithful to the attribute of its corresponding input. In addition, a wavelet packet transform (WPT) module is incorporated to improve the visual fidelity of generated images by capturing age-related texture details at multiple scales in the frequency space. Qualitative results demonstrate the ability of our model in synthesizing visually plausible face images, and extensive quantitative evaluation results show that the proposed method achieves state-of-the-art performance on existing datasets.

## [Automatic Face Aging in Videos via Deep Reinforcement Learning](http://openaccess.thecvf.com/content_CVPR_2019/html/Duong_Automatic_Face_Aging_in_Videos_via_Deep_Reinforcement_Learning_CVPR_2019_paper.html)
> [Chi Nhan Duong](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Khoa Luu](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Kha Gia Quach](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Nghia Nguyen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Eric Patterson](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Tien D. Bui](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Ngan Le](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Duong_Automatic_Face_Aging_in_Videos_via_Deep_Reinforcement_Learning_CVPR_2019_paper.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> This paper presents a novel approach for synthesizing automatically age-progressed facial images in video sequences using Deep Reinforcement Learning. The proposed method models facial structures and the longitudinal face-aging process of given subjects coherently across video frames. The approach is optimized using a long-term reward, Reinforcement Learning function with deep feature extraction from Deep Convolutional Neural Network. Unlike previous age-progression methods that are only able to synthesize an aged likeness of a face from a single input image, the proposed approach is capable of age-progressing facial likenesses in videos with consistently synthesized facial features across frames. In addition, the deep reinforcement learning method guarantees preservation of the visual identity of input faces after age-progression. Results on videos of our new collected aging face AGFW-v2 database demonstrate the advantages of the proposed solution in terms of both quality of age-progressed faces, temporal smoothness, and cross-age face verification.

## [Semantic Component Decomposition for Face Attribute Manipulation](http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Semantic_Component_Decomposition_for_Face_Attribute_Manipulation_CVPR_2019_paper.html)
> [Ying-Cong Chen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Xiaohui Shen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Zhe Lin](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Xin Lu](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[I-Ming Pao](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Jiaya Jia](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Semantic_Component_Decomposition_for_Face_Attribute_Manipulation_CVPR_2019_paper.pdf), [supp](http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Chen_Semantic_Component_Decomposition_CVPR_2019_supplemental.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> Deep neural network-based methods were proposed for face attribute manipulation. There still exist, however, two major issues, i.e., insufficient visual quality (or resolution) of the results and lack of user control. They limit the applicability of existing methods since users may have different editing preference on facial attributes. In this paper, we address these issues by proposing a semantic component model. The model decomposes a facial attribute into multiple semantic components, each corresponds to a specific face region. This not only allows for user control of edit strength on different parts based on their preference, but also makes it effective to remove unwanted edit effect. Further, each semantic component is composed of two fundamental elements, which determine the edit effect and region respectively. This property provides fine interactive control. As shown in experiments, our model not only produces high-quality results, but also allows effective user interaction.

## [Face-Focused Cross-Stream Network for Deception Detection in Videos](http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Face-Focused_Cross-Stream_Network_for_Deception_Detection_in_Videos_CVPR_2019_paper.html)
> [Mingyu Ding](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[An Zhao](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Zhiwu Lu](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Tao Xiang](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Ji-Rong Wen](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Ding_Face-Focused_Cross-Stream_Network_for_Deception_Detection_in_Videos_CVPR_2019_paper.pdf), [supp](http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Ding_Face-Focused_Cross-Stream_Network_CVPR_2019_supplemental.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> Automated deception detection (ADD) from real-life videos is a challenging task. It specifically needs to address two problems: (1) Both face and body contain useful cues regarding whether a subject is deceptive. How to effectively fuse the two is thus key to the effectiveness of an ADD model. (2) Real-life deceptive samples are hard to collect; learning with limited training data thus challenges most deep learning based ADD models. In this work, both problems are addressed. Specifically, for face-body multimodal learning, a novel face-focused cross-stream network (FFCSN) is proposed. It differs significantly from the popular two-stream networks in that: (a) face detection is added into the spatial stream to capture the facial expressions explicitly, and (b) correlation learning is performed across the spatial and temporal streams for joint deep feature learning across both face and body. To address the training data scarcity problem, our FFCSN model is trained with both meta learning and adversarial learning. Extensive experiments show that our FFCSN model achieves state-of-the-art results. Further, the proposed FFCSN model as well as its robust training strategy are shown to be generally applicable to other human-centric video analysis tasks such as emotion recognition from user-generated videos.

## [Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss](http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Hierarchical_Cross-Modal_Talking_Face_Generation_With_Dynamic_Pixel-Wise_Loss_CVPR_2019_paper.html)
> [Lele Chen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Ross K. Maddox](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Zhiyao Duan](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Chenliang Xu](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hierarchical_Cross-Modal_Talking_Face_Generation_With_Dynamic_Pixel-Wise_Loss_CVPR_2019_paper.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism. Furthermore, to generate a sharper image with well-synchronized facial movements, we propose a novel regression-based discriminator structure, which considers sequence-level information along with frame-level information. Thoughtful experiments on several datasets and real-world samples demonstrate significantly better results obtained by our method than the state-of-the-art methods in both quantitative and qualitative comparisons.

# Expression Transfer

## [3D Guided Fine-Grained Face Manipulation](http://openaccess.thecvf.com/content_CVPR_2019/html/Geng_3D_Guided_Fine-Grained_Face_Manipulation_CVPR_2019_paper.html)
> [Zhenglin Geng](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Chen Cao](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Sergey Tulyakov](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Geng_3D_Guided_Fine-Grained_Face_Manipulation_CVPR_2019_paper.pdf), [supp](http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Geng_3D_Guided_Fine-Grained_CVPR_2019_supplemental.zip)

> We present a method for fine-grained face manipulation. Given a face image with an arbitrary expression, our method can synthesize another arbitrary expression by the same person. This is achieved by first fitting a 3D face model and then disentangling the face into a texture and a shape. We then learn different networks in these two spaces. In the texture space, we use a conditional generative network to change the appearance, and carefully design input formats and loss functions to achieve the best results. In the shape space, we use a fully connected network to predict the accurate shapes and use the available depth data for supervision. Both networks are conditioned on expression coefficients rather than discrete labels, allowing us to generate an unlimited amount of expressions. We show the superiority of this disentangling approach through both quantitative and qualitative studies. In a user study, our method is preferred in 85% of cases when compared to the most recent work. When compared to the ground truth, annotators cannot reliably distinguish between our synthesized images and real images, preferring our method in 53% of the cases.

## [Disentangled Representation Learning for 3D Face Shape](http://openaccess.thecvf.com/content_CVPR_2019/html/Jiang_Disentangled_Representation_Learning_for_3D_Face_Shape_CVPR_2019_paper.html)
> [Zi-Hang Jiang](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Qianyi Wu](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Keyu Chen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Juyong Zhang](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Jiang_Disentangled_Representation_Learning_for_3D_Face_Shape_CVPR_2019_paper.pdf), [supp](http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Jiang_Disentangled_Representation_Learning_CVPR_2019_supplemental.pdf), [bibtex](https://zhuanlan.zhihu.com/p/71732523/edit)

> In this paper, we present a novel strategy to design disentangled 3D face shape representation. Specifically, a given 3D face shape is decomposed into identity part and expression part, which are both encoded and decoded in a nonlinear way. To solve this problem, we propose an attribute decomposition framework for 3D face mesh. To better represent face shapes which are usually nonlinear deformed between each other, the face shapes are represented by a vertex based deformation representation rather than Euclidean coordinates. The experimental results demonstrate that our method has better performance than existing methods on decomposing the identity and expression parts. Moreover, more natural expression transfer results can be achieved with our method than existing methods.

# Style Transfer

## [APDrawingGAN: Generating Artistic Portrait Drawings From Face Photos With Hierarchical GANs](http://openaccess.thecvf.com/content_CVPR_2019/html/Yi_APDrawingGAN_Generating_Artistic_Portrait_Drawings_From_Face_Photos_With_Hierarchical_CVPR_2019_paper.html)
> [Ran Yi](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Yong-Jin Liu](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Yu-Kun Lai](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Paul L. Rosin](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yi_APDrawingGAN_Generating_Artistic_Portrait_Drawings_From_Face_Photos_With_Hierarchical_CVPR_2019_paper.pdf), [supp](http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Yi_APDrawingGAN_Generating_Artistic_CVPR_2019_supplemental.pdf)

> Significant progress has been made with image stylization using deep learning, especially with generative adversarial networks (GANs). However, existing methods fail to produce high quality artistic portrait drawings. Such drawings have a highly abstract style, containing a sparse set of continuous graphical elements such as lines, and so small artifacts are much more exposed than for painting styles. Moreover, artists tend to use different strategies to draw different facial features and the lines drawn are only loosely related to obvious image features. To address these challenges, we propose APDrawingGAN, a novel GAN based architecture that builds upon hierarchical generators and discriminators combining both a global network (for images as a whole) and local networks (for individual facial regions). This allows dedicated drawing strategies to be learned for different facial features. Since artists' drawings may not have lines perfectly aligned with image features, we develop a novel loss to measure similarity between generated and artists' drawings based on distance transforms, leading to improved strokes in portrait drawing. To train APDrawingGAN, we construct an artistic drawing dataset containing high-resolution portrait photos and corresponding professional artistic drawings. Extensive experiments, including a user study, show that APDrawingGAN produces significantly better artistic drawings than state-of-the-art methods.

# Other

## [Face Parsing With RoI Tanh-Warping](http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Face_Parsing_With_RoI_Tanh-Warping_CVPR_2019_paper.html)
> [Jinpeng Lin](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Hao Yang](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Dong Chen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Ming Zeng](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Fang Wen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Lu Yuan](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Lin_Face_Parsing_With_RoI_Tanh-Warping_CVPR_2019_paper.pdf)

> Face parsing computes pixel-wise label maps for different semantic components (e.g., hair, mouth, eyes) from face images. Existing face parsing literature have illustrated significant advantages by focusing on individual regions of interest (RoIs) for faces and facial components. However,the traditional crop-and-resize focusing mechanism ignores all contextual area outside the RoIs, and thus is not suitable when the component area is unpredictable, e.g. hair. Inspired by the physiological vision system of human, we propose a novel RoI Tanh-warping operator that combines the central vision and the peripheral vision together. It addresses the dilemma between a limited sized RoI for focusing and an unpredictable area of surrounding context for peripheral information. To this end, we propose a novel hybrid convolutional neural network for face parsing. It uses hierarchical local based method for inner facial components and global methods for outer facial components. The whole framework is simple and principled, and can be trained end-to-end. To facilitate future research of face parsing, we also manually relabel the training data of the HELEN dataset and will make it public. Experiments on both HELEN and LFW-PL benchmarks demonstrate that our method surpasses state-of-the-art methods.

## [Linkage Based Face Clustering via Graph Convolution Network](http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Linkage_Based_Face_Clustering_via_Graph_Convolution_Network_CVPR_2019_paper.html)
> [Zhongdao Wang](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Liang Zheng](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Yali Li](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Shengjin Wang](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Linkage_Based_Face_Clustering_via_Graph_Convolution_Network_CVPR_2019_paper.pdf)

> In this paper, we present an accurate and scalable approach to the face clustering task. We aim at grouping a set of faces by their potential identities. We formulate this task as a link prediction problem: a link exists between two faces if they are of the same identity. The key idea is that we find the local context in the feature space around an instance (face) contains rich information about the linkage relationship between this instance and its neighbors. By constructing sub-graphs around each instance as input data, which depict the local context, we utilize the graph convolution network (GCN) to perform reasoning and infer the likelihood of linkage between pairs in the sub-graphs. Experiments show that our method is more robust to the complex distribution of faces than conventional methods, yielding favorably comparable results to state-of-the-art methods on standard face clustering benchmarks, and is scalable to large datasets. Furthermore, we show that the proposed method does not need the number of clusters as prior, is aware of noises and outliers, and can be extended to a multi-view version for more accurate clustering accuracy.

## [Learning to Cluster Faces on an Affinity Graph](http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Learning_to_Cluster_Faces_on_an_Affinity_Graph_CVPR_2019_paper.html)
> [Lei Yang](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Xiaohang Zhan](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Dapeng Chen](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Junjie Yan](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Chen Change Loy](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Dahua Lin](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Learning_to_Cluster_Faces_on_an_Affinity_Graph_CVPR_2019_paper.pdf)

> Face recognition sees remarkable progress in recent years, and its performance has reached a very high level. Taking it to a next level requires substantially larger data, which would involve prohibitive annotation cost. Hence, exploiting unlabeled data becomes an appealing alternative. Recent works have shown that clustering unlabeled faces is a promising approach, often leading to notable performance gains. Yet, how to effectively cluster, especially on a large-scale (i.e. million-level or above) dataset, remains an open question. A key challenge lies in the complex variations of cluster patterns, which make it difficult for conventional clustering methods to meet the needed accuracy. This work explores a novel approach, namely, learning to cluster instead of relying on hand-crafted criteria. Specifically, we propose a framework based on graph convolutional network, which combines a detection and a segmentation module to pinpoint face clusters. Experiments show that our method yields significantly more accurate face clusters, which, as a result, also lead to further performance gain in face recognition.

## [Pluralistic Image Completion](http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Pluralistic_Image_Completion_CVPR_2019_paper.html)
> [Chuanxia Zheng](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Tat-Jen Cham](http://openaccess.thecvf.com/CVPR2019_search.py%2523),[Jianfei Cai](http://openaccess.thecvf.com/CVPR2019_search.py%2523)

> [pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Pluralistic_Image_Completion_CVPR_2019_paper.pdf), [supp](http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Zheng_Pluralistic_Image_Completion_CVPR_2019_supplemental.pdf)

> Most image completion methods produce only one result for each masked input, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion - the task of generating multiple and diverse plausible solutions for image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label. As such, sampling from conditional VAEs still leads to minimal diversity. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one given ground truth to get prior distribution of missing parts and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the reconstructive path. Both are supported by GANs. We also introduce a new short+long term attention layer that exploits distant relations among decoder and encoder features, improving appearance consistency. When tested on datasets with buildings (Paris), faces (CelebA-HQ), and natural images (ImageNet), our method not only generated higherquality completion results, but also with multiple and diverse plausible outputs.

